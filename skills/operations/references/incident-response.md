# インシデント対応ガイド

## インシデント重大度レベル

| レベル | 説明 | 対応時間 | 例 |
|--------|------|----------|-----|
| **SEV1** | サービス全停止 | 即時 | 全ユーザーがアクセス不可 |
| **SEV2** | 主要機能停止 | 30分以内 | 決済機能が動作しない |
| **SEV3** | 一部機能低下 | 2時間以内 | 検索が遅い |
| **SEV4** | 軽微な問題 | 24時間以内 | UIの表示崩れ |

## インシデント対応フロー

```
┌─────────┐    ┌──────────┐    ┌─────────┐    ┌─────────┐    ┌──────────────┐
│  検知   │ → │ トリアージ │ → │  対応   │ → │  復旧   │ → │ポストモーテム│
└─────────┘    └──────────┘    └─────────┘    └─────────┘    └──────────────┘
```

## フェーズ1: 検知

### 検知源
- モニタリングアラート
- ユーザー報告
- 自動テスト失敗
- ログ異常

### 初動確認
```bash
# サービス状態確認
curl -I https://example.com/health

# ログ確認
kubectl logs -f deployment/myapp --tail=100

# メトリクス確認
# Grafanaダッシュボードを確認
```

## フェーズ2: トリアージ

### 影響範囲の特定
- 影響を受けているユーザー数
- 影響を受けている機能
- 地理的な影響範囲

### 重大度判定
```
Q1: サービスは完全に停止している？
  Yes → SEV1
  No → Q2

Q2: 主要な収益機能に影響がある？
  Yes → SEV2
  No → Q3

Q3: ユーザー体験に明らかな影響がある？
  Yes → SEV3
  No → SEV4
```

### エスカレーション
```
SEV1: オンコール担当 + エンジニアリングマネージャー + 経営陣
SEV2: オンコール担当 + チームリード
SEV3: オンコール担当
SEV4: 通常のチケット処理
```

## フェーズ3: 対応

### コミュニケーション

```markdown
# インシデント通知テンプレート

**インシデント**: [タイトル]
**重大度**: SEV[X]
**ステータス**: 調査中 / 対応中 / 解決済み

**影響**:
- [影響の説明]

**タイムライン**:
- HH:MM - 検知
- HH:MM - 対応開始

**対応チーム**:
- インシデントコマンダー: @名前
- 技術リード: @名前

**次回更新**: [時間]
```

### 一般的な対応手順

#### ロールバック
```bash
# Kubernetes
kubectl rollout undo deployment/myapp

# Cloud Run
gcloud run services update-traffic myapp \
  --to-revisions PREVIOUS=100

# Vercel
vercel rollback
```

#### スケールアップ
```bash
# レプリカ数増加
kubectl scale deployment/myapp --replicas=10

# リソース増加
kubectl set resources deployment/myapp \
  -c myapp \
  --requests=cpu=500m,memory=512Mi \
  --limits=cpu=1000m,memory=1Gi
```

#### 機能フラグでの無効化
```typescript
// 問題のある機能を無効化
if (featureFlags.isEnabled('problematic-feature')) {
  return await newFeature()
} else {
  return await fallback()
}
```

## フェーズ4: 復旧

### 復旧確認
```bash
# ヘルスチェック
curl https://example.com/health

# エラー率確認
# Grafanaで確認

# ユーザー影響確認
# カスタマーサポートと連携
```

### 復旧宣言
```markdown
**インシデント解決通知**

インシデント [タイトル] は解決しました。

**根本原因**: [簡潔な説明]
**解決方法**: [対応内容]
**影響期間**: HH:MM - HH:MM (XX分間)

詳細なポストモーテムは [日時] に実施予定です。
```

## フェーズ5: ポストモーテム

### ポストモーテムテンプレート

```markdown
# インシデントポストモーテム: [タイトル]

**日時**: YYYY-MM-DD HH:MM - HH:MM
**重大度**: SEV[X]
**インシデントコマンダー**: [名前]
**執筆者**: [名前]

## サマリー
[2-3文でインシデントの概要]

## 影響
- 影響を受けたユーザー数: [N]
- 影響を受けた機能: [機能名]
- ダウンタイム: [X分]
- 収益影響: [金額/推定]

## タイムライン (JST)
| 時刻 | イベント |
|------|---------|
| 10:00 | アラート発報 |
| 10:05 | オンコール担当が確認開始 |
| 10:15 | 原因特定、ロールバック開始 |
| 10:20 | ロールバック完了 |
| 10:25 | 復旧確認、インシデントクローズ |

## 根本原因
[技術的な根本原因の説明]

## 検知
- 検知方法: [アラート/ユーザー報告/etc]
- 検知までの時間: [X分]

## 対応
[対応内容の詳細]

## 再発防止策
| アクション | 担当者 | 期限 | ステータス |
|-----------|--------|------|-----------|
| [アクション1] | @名前 | YYYY-MM-DD | 未着手 |
| [アクション2] | @名前 | YYYY-MM-DD | 進行中 |

## 学んだこと
### うまくいったこと
- [項目]

### 改善できること
- [項目]

## 参考リンク
- [アラートリンク]
- [ダッシュボードリンク]
- [関連PR/Issue]
```

### 5 Whys 分析

```
問題: ユーザーがログインできなかった

Why 1: 認証サービスがエラーを返していた
Why 2: データベース接続がタイムアウトしていた
Why 3: データベースのコネクションプールが枯渇していた
Why 4: 新しいデプロイでコネクションリークが発生していた
Why 5: コードレビューでリソース解放漏れを見逃した

根本原因: コードレビューのチェックリストにリソース管理の項目がなかった
再発防止: コードレビューチェックリストにリソース管理を追加
```

## オンコール

### ローテーション設定
```
週次ローテーション
- 主担当: 1名
- バックアップ: 1名
- エスカレーション先: チームリード
```

### オンコールチェックリスト
- [ ] PagerDutyアプリがインストールされている
- [ ] VPN接続できる
- [ ] 本番環境へのアクセス権がある
- [ ] ランブックにアクセスできる
- [ ] Slackの通知が有効

### ランブック例
```markdown
# 高CPU使用率アラート対応

## 確認事項
1. どのPodでCPU使用率が高いか
2. 最近のデプロイがあったか
3. トラフィック増加があったか

## 対応手順
1. メトリクスを確認
   ```
   kubectl top pods -n production
   ```

2. ログを確認
   ```
   kubectl logs -f deployment/myapp --tail=100
   ```

3. 必要に応じてスケールアウト
   ```
   kubectl scale deployment/myapp --replicas=5
   ```

## エスカレーション
30分以内に解決しない場合は @チームリード に連絡
```
